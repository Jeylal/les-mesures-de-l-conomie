---
title: "Introduction aux séries temporelles et à l'analyse de régression"
subtitle: "Séminaire les mesures de l'économie"
author: "Celâl Güney"
format: 
  revealjs:
    theme: style.scss
    scrollable: true
editor: visual
embed-resources: true
lang: fr
---

## Données économiques: principes de base {.smaller}

-   Deux sources principales de données en économie (ainsi qu'en sciences sociales en général)

    1.  Données provenant d'enquêtes au moyen de questionnaires
        -   ex: données sur la consommation, statistiques du marché du travail...
    2.  Données administratives
        -   ex: une partie importante des comptes nationaux, taux d'intérêt, masse monétaire...

Distinction importante, car pour les données d'enquêtes, les principes de la statistique (échantillonage, calculs des marges d'erreurs, tests statistiques) doivent être respectés, ce qui n'est pas (forcément) le cas des données administratives.

::: notes
Car pour les données administratives, certains principe de la statistique ne sont pas applicable, car les données n'ont pas été récoltées à travers un processus d'échantillonnage
:::

## Données d'enquête {.smaller}

-   Important de vous rappeler ce que vous avez appris dans vos cours de méthodes quantitatives
-   Faire attention à la qualité des enquêtes qui dépend de:
    1.  Le procédé d'échantillonage
    2.  La taille de l'échantillon et sa représentativité
    3.  Le taux de réponse (nombre de personnes qui ont répondu à l'enquête / total des personnes appelées à participer à l'enquête)

## Récolte de données d'enquête {.smaller}

#### un problème à ne pas sous-estimer

::::: columns
::: {.column width="50%"}
![](images/cours2/dataproblemft1.png)
:::

::: {.column width="50%"}
[![](images/cours2/dataproblemft2.png)](https://www.ft.com/content/c046be2c-5496-4e2c-b243-580224e9a459)
:::
:::::

Un taux de réponse faible (\<60%) indique que l'enquête a manqué une partie importante de la population ==\> données récoltées ne sont pas représentatives.

## Exemple: crise des données en Grande-Bretagne {.smaller}

:::::: {columns}
::: {.column width="35%"}
"Declining response rates to the LFS (labour force surveys) have made the numbers so volatile that it is impossible to be sure whether employment is rising or falling from one quarter to the next — let alone how the labour market has evolved in the years since the pandemic." "How flawed data is leaving the UK in the dark", Financial Times
:::

:::: {.column width="65%"}
::: r-stack
[![](images/cours2/resolutionfoundation.png){width="692"}](https://www.resolutionfoundation.org/our-work/estimates-of-uk-employment/)

[![](images/cours2/hownotfirestatisticchiefft1.png){.fragment}](https://www.ft.com/content/6eb3c205-c473-47b4-bed8-1b9ee99ce658)
:::
::::
::::::

## Exemple: crise des données aux USA

::::: columns
::: {.column width="50%"}
[![](images/cours2/hownotfirestatisticchiefft4.png)](https://www.ft.com/content/d3b24f17-96d8-4f07-a169-0d22fab051ef)
:::

::: {.column width="50%"}
[![](images/cours2/hownotfirestastisticchief5.png)](https://www.ft.com/content/d3b24f17-96d8-4f07-a169-0d22fab051ef)
:::
:::::

## Données administratives {.smaller}

Exemple: données fiscales (qui permettent les mesures de distribution du revenu), certaines données du marché du travail (ex: Seco et le nombre de chômeurs inscrits), données sur les finances publiques...

-   Les données administratives ne sont pas récoltées à travers un processus d'échantillonnage. Par exemple, cela ne fait pas sens de calculer les intervalles de confiance pour le PIB ou la dette publique.

-   Mais cela ne veut pas dire que les données administratives sont exhaustives

    -   D'ou la récurrence de "révisions" des séries de données administratives.

# Notions de base en analyse de séries temporelles

## Séries temporelles

La plupart des mesures de l'économie sont des séries temporelles (PIB, emploi, inflation...).

L'analyse des séries temporelles présente certaines particularités par rapport aux données en coupe (à un point donné dans le temps, aussi appelé "cross-sectional data"), notamment car elles ont une tendance temporelle (par exemple exponentielle).

## Taux de croissance {.smaller}

Les séries temporelles ont la particularité de croître à un taux plus ou moins stable dans le temps.

Cela implique une croissance exponentielle

Exemple: imaginons une variable $x$ qui croît à un taux constant de 3% par an. A $t = 0$, $x =2$. A $t=1$, x augmente de $2*1.03$ (0.03 étant le taux de croissance auquel on additionne 1):

$x_{t=1} = 2*1.03$

Pour $t=2$, $x_{t=2} = 2*1.03*1.03 = 2*(1.03)^2$

Pour $t=3$, $x_{t=3} = 2*1.03*1.03*1.03 = 2*(1.03)^3$

Ainsi de suite, pour la formule générale:

$x_t = 2*(1.03)^t$. Il s'agit de la formule de croissance exponentielle: $x_t = x_0 (1+g)^t$, avec $x_0$ la valeur initiale, $g$ le taux de croissance et $t$ le nombre de périodes.

## Croissance exponentielle

### Exemple dans R

```{r}
#| echo: true
#| output-location: column-fragment
f = function(x0, g, t){
  
  x0*(1+g)^t
  
}

x0 = 2
g = 0.03
t = 1:150

x = f(x0 = x0, g = g, t = t)

plot(x, type = "l")




```

## Taux de croissance {.smaller}

À partir de la formule pour la croissance exponentielle de X

$$x_t = x_0(1+g)^t$$

on peut trouver le taux de croissance:

$$g = \left(\frac{x_t}{x_0}\right)^{1/t}-1$$

Il s'agit du *taux de croissance moyen composé* de la série entre $x_t$ (en dernière période) et $x_t$

Lorsque l'on calcule le taux de croissance entre deux périodes seulement (disons $x_0$ et $x_1$), on retrouve la formule habituelle du taux de croissance (car $t=1$):

$$
g = \frac{x_1 - x_0}{x_0} = \left(\frac{x_1}{x_0}\right)^{1/1}-1
$$

## Taux de croissance

À ne pas confondre avec la moyenne des taux de croissance:

$$
\bar{g} = \frac{1}{t}\sum_{i=1}^t{g_i}
$$

g et $\bar{g}$ sont identiques seulement si le taux de croissance est constant pour toutes les périodes t.

## Transformation en logarithme {.smaller}

::::: columns
::: column
La transformation en logarithme (passer de $x_t$ à $log(x_t)$) est très courante en économie, surtout avec des séries temporelles caractérisées par une croissance exponentielle.

-   La transformation log permet de "linéariser" une série exponentielle
-   Lorsque l'on visualise la courbe d'une série en logarithme, *la pente est le taux de croissance*
:::

::: column
```{r}
#| echo: true
plot(x, type = "l")
```

```{r}
#| echo: true
plot(log(x), type = "l")
```
:::
:::::

## Transformation en logarithme {.smaller}

Si $x_t = x_0(1+g)^t$ est transformé en log et que nous isolons g, nous trouvons:

$$
log(1+g) = \frac{log(x_t)-log(x_0)}{t}
$$ Entre seulement deux période (par exemple d'une année à l'autre), $t = 1$ et donc le taux de croissance peut être approximé facilement en prenant la différence en log. Quand $g$ est entre -0.15 et 0.15, $log(1+g)$ est une bonne approximation de $g$.

Dans R:

```{r}
#| echo: true

library(tidyverse) # la fonction lag() doit être importée avec le package tidyverse

g = log(x) - lag(log(x)) # lag() prend la valeur de x à t-1

# Ou bien avec diff() (difference):
g = diff(log(x))
```

## Pourquoi log(1+g) ≅ g entre -0.15 et 0.15

```{r}
#| echo: true
#| output-location: column
g = seq(from = -0.30, to = 0.30, by = 0.001)
`log(g+1)` = log(g+1)

df = tibble(g = g, `log(g+1)` = `log(g+1)`)

df %>% 
  ggplot(aes(x = g, y = g, color = "g"))+
  geom_line()+
  geom_line(aes(x = g, y = `log(g+1)`, color = "Log(1+g)"))+
  theme_minimal()+
  labs(x = "", y = "")

```

## Exemple avec des données réelles du PIB

```{r}
#| echo: true
#| output-location: column-fragment

library(maddison) # le package maddison permet d'importer des données de longue durée du PIB par habitant
library(tidyverse) 

data <- subset(maddison, 
             year >= 1800 &
             iso2c %in% c("FR", "US"))

data %>% 
  ggplot(aes(x = year, y = rgdpnapc, color = country)) +
  geom_line()+
  theme_minimal()+
  labs(x = "",
       y = "PIB par habitant (dollar US 2011)",
       title = "Échelle normale (non-logarithmique)")





```

## Exemple avec des données réelles du PIB

```{r}
#| echo: true
#| output-location: column-fragment


data %>% 
  ggplot(aes(x = year, y = log(rgdpnapc), color = country)) +
  geom_line()+
  theme_minimal()+
  labs(x = "",
       y = "PIB par habitant (dollar US 2011)",
       title = "Échelle logarithmique")


```

# Principes de base de l'analyse de régression

## Pourquoi l'analyse de régression ?

-   L'outil principal en économie et en sciences sociales (en méthodes quantitatives)
-   Permet d'estimer des fonctions provenant de théories économiques (fonction de consommation, d'investissement...)
-   Permet d'identifier et d'analyser des relations de causalité
-   Estimation de modèles économiques (par exemple ceux que vous allez voire en macroéconomie)

## Exemple: fonction de consommation {.smaller}

L'une des fonctions les plus connues et estimées en macroéconomie est la fonction de consommation des ménages $C$:

$$
C = c_0 + c_1W
$$

Avec $C$ le niveau de consommation des ménages (dans la monnaie du pays considéré), $c_0$ le niveau de consommation lorsque les salaires $W$ = 0. $c_1$ est la propension marginale à consommer, qui est une composante clé du multiplicateur keynésien.

## Régression linéaire simple {.smaller}

Cette fonction de consommation peut être estimée au moyen d'un modèle de régression linéaire simple. - "Simple", car nous avons uniquement une variable explicative (les salaire $W$). - "linéaire", car notre variable dépendante est numérique (la consommation $C$).

La régression linéaire simple prend la forme suivante:

$$
E[Y|X] = \beta_0 + \beta_1X_i + e_i
$$

Avec $E[Y|X]$ "l'espérance" (la moyenne) de Y, notre variable dépendante, *en fonction* des valeur de notre variable explicative $X$. $\beta_0$ l'ordonnée à l'origine et $e_i$ le terme d'erreur. $E[Y|X]$ est souvent écrit $Y$ par simplicité.

==\> Les régressions peuvent paraître compliquées à première vue ==\> Une manière simple de "démistifer" les régression est de comprendre qu'il s'agit simplement de *comparaisons de moyennes*

## $$E[Y|X] = \beta_0 + \beta_1X_i + e_i$$ {.smaller}

$\beta_1$ est le paramètre clé du modèle.

L'interprétation la plus courante de ce paramètre, qui est celle qui vous est enseignée dans vos cours de quanti, est que $\beta_1$ est l'effet de X sur les valeurs moyennes Y lorsque X augmente de une unité Cela veut dire que $\beta_1$ est l'effet marginal de x sur y, la valeur obtenue en prenant la dérivée de Y sur X:

$$
\frac{\delta E[Y|X]}{\delta X} = \beta_1
$$

==\> Il s'agit de l'interprétation causale du coefficient du réggression ($\beta_1$)

## Interprétation en termes de comparaison {.smaller}

Le coefficient $\beta_1$ peut être aussi compris comme une différence de moyenne prédite par le modèle entre deux valeurs de X.

-   Si $X$ est une variable qualitative dichotomique (par exemple homme ou femme), elle ne prend que les valeurs 0 (disons homme) et 1 (disons femme) et que nous ignorons le termes d'erreur $e_i$:

Si $X = 1$, \<=\> $E[Y|X = 1] = \beta_0 + \beta_1$

Et si $X = 0$, \<=\> $E[Y|X = 0] = \beta_0$

Et donc $$E[Y|X=1] - E[Y|X=0] = \beta_0 + \beta_1 - \beta_0 = \beta_1$$

$\beta_1$ est la différence entre la moyenne prédite des femmes et des hommes.

## Interprétation en termes de comparaison {.smaller}

Même principe si la variable est numérique/continue

Exemple avec $X_i$ le revenu de la personne $i$ et $Y=C$ la consommation. Prenons $X_1 = 5000$ et $X_2 = 5001$. Le coefficient $\beta1$ est la différence entre $X_2$ et $X_1$.

$$E[Y|X_2 = 5001] - E[Y|X_1 = 5000] = \beta_0 + \beta_15001 - \beta_0 - \beta_15000 = \beta_1$$

$\beta_1$ est donc la différence de consommation moyenne prédite par le modèle entre deux personnes ayant un revenu de 5001 et 5000. En d'autre termes, la différence de moyenne prédite entre deux personnes différant d'une unité.

## Interprétation des coefficients de régression {.smaller}

1.  Interprétation causale: $\beta_1$ est l'effet marginal de X (effet lorsque X augmente de 1) sur les valeurs moyennes prédites de Y.

2.  Interprétation en termes de comparaison: $\beta_1$ est la différence des moyennes prédites de Y entre deux observations différant d'une unité.

==\> L'interprétation en termes de comparaison est plus prudente que l'interprétation causale, pour laquelle il faut que les hypothèses du modèle soient respectées

## Retour à notre exemple $$C = c_0 + c_1W$$

```{r}
#| echo: true
#| output-location: column

library(readr)
macroch <- read_csv("data/macroch.csv")

macroch %>% 
  ggplot(aes(x = W_real, y = C_real))+
  geom_point()+
  theme_minimal()+
  labs(x = "Rémunération des salariés (W)",
       y = "Consommation (C)")

```

## Estimation simple avec lm()

```{r}
#| echo: true
#| output-location: default

library(kableExtra)
library(broom)
model = lm(data = macroch, # préciser le tableau dans lequel les variables sont présentes
           formula = C_real ~ W_real) # la formule estimée doit prendre la forme Y ~ X

tidy(model) %>% 
  kable() 

```

La fonction estimée est donc:

$$C = 43 + 0.797W$$

## Valeurs prédites {.smaller}

```{r}
#| echo: true
#| output-location: column
data = augment(model)

data %>% 
  ggplot(aes(x = W_real, y = C_real, color = "Valeurs réelles observées"))+
  geom_line()+
  geom_line(aes(x = W_real, y = .fitted, color = "Valeurs prédites"))+
  theme_minimal()+
  labs(title = "C = 43 + 0.797W")


```

## Vérification des hypothèses du modèle {.smaller}

1. Linéarité et indépendance des erreurs

```{r}
#| echo: true
#| output-location: column
library(performance)

diagnostic_plots = plot(check_model(model, panel = FALSE))
diagnostic_plots[[2]]
```

On voit clairement un pattern dans la distribution des données ==> pas d'indépendance des erreurs, ce qui est typiquement le cas des séries temporelles

## Vérification des hypothèses du modèle {.smaller}

2. Homoscédasticité

```{r}
#| echo: true
library(performance)

diagnostic_plots[[3]]
```

Il y aussi clairement un problème d'hétéroscédasticité: la variance des erreurs n'est pas constante

## $C = 43 + 0.797W$ {.smaller}

Le plus souvent, régresser tel quel des séries temporelles (en niveau) va produire des modèles qui ne respectent pas l'hypothèse fondamentale d'indépendance des erreurs. Cela est un problème, car implique que notre coefficient de $\beta_1 = 0.79$ est *biaisé*.

==> Cela vient du fait qu'il a une dimension temporelle dans nos variables, ces dernières sont aussi une fonction du temps $t$, qui n'est pas prise en compte dans $C = c_0 +c_1W + e_i$ (enfait, la dimension temporelle va se voire dans le terme d'erreur $e_i$)

==> Autrement pour pouvoir modéliser des séries temporelles avec des régressions, il faut que ces séries soient "stationnaires", c-à-d que leur moyenne doit être stable dans le temps.

Il existe des tests afin de tester la stationnarité (Test de Dickey-Fuller, test de Perron...)

## Stationnarité {.smaller}

La plupart du temps, les séries ayant une tendance temporelle (non-stationnaire) doivent être transformées en taux de croissance pour pouvoir être incluses dans une régression.

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Afficher le code"

library(ggcharts)

macroch$year = 1960:2022
data_rates = tibble(year = 1961:2022, rate_c = diff(log(macroch$C_real)), rate_w = diff(log(macroch$W_real)))

plot1 <- 
macroch %>% 
  ggplot(aes(x = year, y = C_real, color = "Consommation (niveau)"))+
  geom_line()+
  geom_line(aes(x = year, y = W_real, color = "Revenu des salariés (niveau)"))+
  theme_minimal()+
  labs(title = "Séries non stationnaires")+
  theme(legend.position = "top")



plot2 <- 
  data_rates %>%
  ggplot(aes(x = year, y = rate_c, color = "Consommation (taux de croissance)"))+
  geom_line()+
  labs(title = "Série stationnaire")+
  theme_minimal()+
  theme(legend.position = "top")

cowplot::plot_grid(plot1, plot2)

```


## Modèle en différences de premier ordre {.smaller}

==> régresser les taux de croissances au lieu des niveaux, pour corriger la non-stationnarité.

Au lieu de $C = \beta_0 + \beta_1W$,
nous avons:
$\%\Delta{C} = \beta_0 + \%\Delta\beta1W$

$\beta_1$ devient *l'élasticité* de la consommation par rapport au revenu: le pourcentage de changement de C quand W augmente de 1 point de pourcentage.

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Afficher le code"

model3 = lm(data = data_rates,
            rate_c ~ rate_w)

tidy(model3)


```


## Régression d'une série temporelle sur sa trend {.smaller}

Que se passe-t-il si notre régression inclue seulement la dimension temporelle comme variable explicative ?



























